name: ccloco_muon_h_30_sweep
project:  ccloco500M
method: grid
program: train.py
command:
  - /usr/bin/env
  - torchrun
  - --nproc_per_node=8
  - ${program} 
  - ${args_no_boolean_flags}
  - --track_muon_rms

parameters:
  # Run configuration
  strategy:
    value: ccloco_muon
  run_name:
    value: ccloco_muon_h_30_lr_001_and_00006

  # Data configuration
  shards_path:
    value: $DATA_DIR/dclm_tokenized
  token_budget:
    value: 10255073280 # [effective_bs=2**22 * H=15 * iterations=163]
  shard_token_size:
    value: 1073741824 # 1Gi
  sequence_length:
    value: 2048

  # Model configuration
  hparams_file:
    value: hparams/500M/500M_model_hparams.json
  use_compile:
    value: True

  # Training configuration
  micro_batch_size:
    value: 32 # -1 to set micro_batch_size to batch_size
  batch_size:
    values: 
    - 256
  outer_learning_rate:
    values:
    - 1
  inner_learning_rate:
    # Re: AdamW learning rate for embedding/head/scalar params
    # The question is how to scale adam w.r.t. muon updates.
    # Here we match the adam lr with muon, and then scale it down per param group. 
    # We use the same scale for all adam params because adam is adaptive per-param anyways
    values:
    # - 0.02  # Original Muon learning rate
    # - 0.01
  # will uncomment in subsequent commit, to be able to run in parallel, and track in wandb.. meanwhile will check if there is another way?
    - 0.001 
    - 0.0006
  # Muon-specific hyperparameters
  muon_adam_lr_scale:
    values:
    - 0.05
    - 0.5
    - 1.0
  muon_momentum:
    value: 0.95  # Muon momentum
  muon_weight_decay:
    values:
    - 0.01  # Original Muon weight decay
  inner_steps:
    values:
    - 30
  warmup_steps:
    value: 500
  weight_decay:
    values: 
    - 0.1  # AdamW weight decay for embedding/head params
  outer_nesterov:
    value: False # later can explore aligning with muon which has this True by default, for now, we will not

  # Compression configuration
  error_decay:
    value: 0.95
  top_k:
    value: 32
  chunk_size:
    value: 64
